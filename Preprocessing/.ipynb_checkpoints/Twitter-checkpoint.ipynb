{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "paperback-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import flair\n",
    "import os\n",
    "import datetime\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(input_filename, output_filename):\n",
    "    #opening data and filtering used feature\n",
    "    df = pd.read_csv(input_filename)\n",
    "    df = pd.DataFrame(df)\n",
    "    cols = [3,4,10]\n",
    "    df = df[df.columns[cols]]\n",
    "    df.columns=[\"Date\", \"Time\", \"Tweet\"]\n",
    "    df = df.iloc[1: , :]\n",
    "    df['Timestamp']=df.apply(lambda x:'%s %s' % (x['Date'],x['Time']),axis=1)\n",
    "    df = df.drop(['Date', 'Time'], axis = 1)\n",
    "    old_cols = df.columns.values \n",
    "    new_cols= ['Timestamp', 'Tweet']\n",
    "    df = df.reindex(columns=new_cols)\n",
    "    \n",
    "    #cleaning tweet\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    clean_tweets = []\n",
    "    for tweet in df['Tweet']:\n",
    "        # remove http links\n",
    "        tweet = re.sub(r\"(http|https)://\\S+\", \"\", tweet)\n",
    "        # remove punctuation\n",
    "        tweet = re.sub(r'[^\\w\\s]','', tweet)\n",
    "        # remove numbers\n",
    "        tweet = re.sub(r'\\d+','', tweet)\n",
    "        # remove coin symbol\n",
    "        tweet = re.sub(r\"(btc|eth|ltc|ico)\",\"\", tweet)\n",
    "\n",
    "        # now remove stopwords\n",
    "        tweet = tweet.split()\n",
    "        tweet = [w for w in tweet if not w in stopwords]\n",
    "        tweet = \" \".join(word for word in tweet)\n",
    "        clean_tweets.append(tweet)\n",
    "    df['Clean Tweet'] = clean_tweets\n",
    "    df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jewish-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-09 18:59:03,909 loading file C:\\Users\\azkaz\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "fmt = '%Y-%m-%d %H:00:00'\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setiment_analyze(input_filename, output_filename):\n",
    "    #opening cleaned data\n",
    "    df = pd.read_csv(input_filename)\n",
    "    df = df[['Cleaned Tweets','Timestamp']]\n",
    "    df = df.fillna('')\n",
    "    df = df.iloc[: , :]\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    #analyze the tweet using VADER\n",
    "    for row_i, row in df.iterrows():\n",
    "\n",
    "        pos_dict = dict()\n",
    "        neg_dict = dict()\n",
    "\n",
    "        data = row['Cleaned Tweets']\n",
    "        print(row_i)\n",
    "        print(data[0:15])\n",
    "        ss = sid.polarity_scores(data)\n",
    "        pos_dict[str(row_i)] = ss['pos']\n",
    "        neg_dict[str(row_i)] = ss['neg']\n",
    "\n",
    "        pos_df = pd.DataFrame.from_dict(pos_dict, orient='index',\n",
    "                                            columns=['twitter_positifitas'])\n",
    "        pos_df.index.name = 'timestamp'\n",
    "\n",
    "        neg_df = pd.DataFrame.from_dict(neg_dict, orient='index',\n",
    "                                            columns=['twitter_negatifitas'])\n",
    "        neg_df.index.name = 'timestamp'\n",
    "\n",
    "        final_senti_df = pd.concat([pos_df, neg_df], axis=1)\n",
    "\n",
    "        if os.path.exists(output_filename):\n",
    "            keep_header = False\n",
    "        else:\n",
    "            keep_header = True\n",
    "\n",
    "        final_senti_df.to_csv(output_filename, mode='a', header=keep_header)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sentiment(input_filename, output_filename):\n",
    "    start_date_time_obj = datetime.datetime(2021, 8, 1)\n",
    "    end_date_time_obj = datetime.datetime(2021, 8, 2)\n",
    "    hr1 = datetime.timedelta(hours=1)\n",
    "    curr_date_time_obj = start_date_time_obj\n",
    "    in_df = pd.read_csv(input_filename)\n",
    "\n",
    "\n",
    "    out_dict = dict()\n",
    "\n",
    "    while curr_date_time_obj <= end_date_time_obj:\n",
    "        curr_timestamp = curr_date_time_obj.strftime(format=fmt)\n",
    "        out_dict[curr_timestamp] = 0\n",
    "        curr_date_time_obj += hr1\n",
    "\n",
    "    out_df = pd.DataFrame.from_dict(out_dict, orient='index',\n",
    "                                    columns=['twitter_flair'])\n",
    "\n",
    "    print(out_dict)\n",
    "    out_df.index.name = 'timestamp'\n",
    "    # menambahkan kolom kosong\n",
    "    out_df['twitter_positifitas'] = 0\n",
    "    out_df['twitter_negatifitas'] = 0\n",
    "\n",
    "    for i in range(len(in_df)):\n",
    "        timestamp = in_df.loc[i, 'timestamp']\n",
    "        out_key = datetime.datetime.strptime(str(timestamp), '%Y-%m-%d %H:%M:%S')\n",
    "        out_key += hr1\n",
    "        out_key = out_key.strftime(format='%Y-%m-%d %H:00:00')\n",
    "        # adding data count to count all the data\n",
    "        try:\n",
    "            out_df.loc[out_key, 'twitter_positifitas'] += in_df.loc[i, 'twitter_positifitas']\n",
    "            out_df.loc[out_key, 'twitter_negatifitas'] += in_df.loc[i, 'twitter_negatifitas']\n",
    "            out_df.loc[out_key, 'count'] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # make timestamp as a column and reindex the dataframe to make loc method happy\n",
    "    out_df['timestamp'] = out_df.index\n",
    "    out_df.index = range(len(out_df))\n",
    "\n",
    "    for i in range(len(out_df)):\n",
    "        # normalize the value of sentiment analysis according to the total each hour\n",
    "\n",
    "        if out_df.loc[i, 'twitter_sid_count'] == 0:\n",
    "            out_df.loc[i, 'twitter_positifitas'] = 0\n",
    "            out_df.loc[i, 'twitter_negatifitas'] = 0\n",
    "        else:\n",
    "            out_df.loc[i, 'twitter_positifitas'] /= out_df.loc[i, 'twitter_sid_count']\n",
    "            out_df.loc[i, 'twitter_negatifitas'] /= out_df.loc[i, 'twitter_sid_count']\n",
    "\n",
    "        if os.path.exists(output_filename):\n",
    "            keep_header = False\n",
    "        else:\n",
    "            keep_header = True\n",
    "\n",
    "    out_df.drop(['twitter_flair_count', 'twitter_tb_polarity_count', 'twitter_tb_subjectivity_count','twitter_sid_count'], axis=1,\n",
    "                inplace=True)\n",
    "    # change back index to timestamp to save the data in csv\n",
    "    out_df.set_index('timestamp', inplace=True)\n",
    "    out_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-enough",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_filename = 'twitter_data.csv'\n",
    "    output_cleaned_filename = input_filename[0:-4] + '_cleaned.csv'\n",
    "    \n",
    "    # Cleaning tweets\n",
    "    cleaning_data(input_filename, output_cleaned_filename)\n",
    "    output_sentiment_filename = input_filename[0:-4] + '_sentiment.csv'\n",
    "    \n",
    "    # Sentiment analysis of the tweet\n",
    "    setiment_analyze(output_cleaned_filename, output_sentiment_filename)\n",
    "    output_categorize_sentiment_filename = output_sentiment_filename[0:-4] + '_bucketized.csv'\n",
    "\n",
    "    # Get all sentiment reports and categorize them into hourly basis data according to the timestamp\n",
    "    categorize_sentiment(output_sentiment_filename, output_categorize_sentiment_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-handling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
